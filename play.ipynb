{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83257b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VSIBLE_DEVICES\"] = '0'\n",
    "import torch\n",
    "from diffusers import AutoencoderKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba8365",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.load('./checkpoints/vae/best.pth')\n",
    "print(s['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vae(vae_latent_channels=4):\n",
    "    vae = AutoencoderKL(\n",
    "        sample_size=128,\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        down_block_types=(\"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\"),\n",
    "        up_block_types=(\"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\"),\n",
    "        block_out_channels=(64, 128, 256),\n",
    "        latent_channels=vae_latent_channels,\n",
    "        scaling_factor=0.057867,\n",
    "    )\n",
    "    return vae\n",
    "\n",
    "\n",
    "def try_encode_latent(vae, imgs):\n",
    "    enc = vae.encode(imgs)\n",
    "    if hasattr(enc, 'latent_dist'):\n",
    "        try:\n",
    "            lat = enc.latent_dist.sample()\n",
    "        except Exception:\n",
    "            lat = enc.latent_dist.mean\n",
    "    elif isinstance(enc, dict):\n",
    "        if 'latent_dist' in enc:\n",
    "            try:\n",
    "                lat = enc['latent_dist'].sample()\n",
    "            except Exception:\n",
    "                lat = enc['latent_dist'].mean\n",
    "        elif 'sample' in enc:\n",
    "            lat = enc['sample']\n",
    "        else:\n",
    "            lat = torch.as_tensor(enc)\n",
    "    else:\n",
    "        lat = enc\n",
    "    return lat\n",
    "\n",
    "\n",
    "def try_decode(vae, latents):\n",
    "    out = vae.decode(latents)\n",
    "    if isinstance(out, dict):\n",
    "        if 'sample' in out:\n",
    "            return out['sample']\n",
    "        for v in out.values():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                return v\n",
    "        raise RuntimeError(\"Unexpected vae.decode() return structure.\")\n",
    "    elif isinstance(out, torch.Tensor):\n",
    "        return out\n",
    "    else:\n",
    "        return torch.as_tensor(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ecc6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "vae = AutoencoderKL(\n",
    "    sample_size=128,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    down_block_types=(\"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\"),\n",
    "    up_block_types=(\"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\"),\n",
    "    block_out_channels=(64, 128, 256),\n",
    "    latent_channels=4,\n",
    "    scaling_factor=0.057867,\n",
    ").cuda()\n",
    "vae.load_state_dict(s['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vae.config.scaling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a914b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "from PIL import Image\n",
    "path = ['./data/gen_msrs/00004N.png']\n",
    "vis = [Image.open(p).convert(\"RGB\") for p in path]\n",
    "vis[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tensor [-1,1]\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "transform = Compose([ToTensor(), Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "vis = [transform(image).unsqueeze(0).cuda() for image in vis]\n",
    "vis = torch.cat(vis)\n",
    "enc = vae.encode(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ef520",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.latent_dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefbe0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = vae.decode(enc.latent_dist.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a939335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc2 = try_encode_latent(vae, vis)\n",
    "# try_decode(vae, )\n",
    "isinstance(out, dict)\n",
    "out['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee85669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还原out图片\n",
    "import numpy as np\n",
    "vi = Image.open(path[0]).convert(\"RGB\")\n",
    "image_array = out['sample'].detach().cpu().numpy()[0]\n",
    "# print(image_array.shape) # (3, 480, 640)\n",
    "image_array = image_array.transpose(1, 2, 0)\n",
    "# [-1,1] to [0, 255]\n",
    "image_array = (image_array + 1.0) * 127.5\n",
    "# clip to 0 to 255\n",
    "image_array = np.clip(image_array, 0, 255)\n",
    "vi1 = Image.fromarray(image_array.astype(np.uint8), 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce82d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9dff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58f693",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 新增 notebook cell：加载已保存权重，用 pipeline 进行 train/test 推理，打印图与指标，并对训练集计算一次 proxy loss\n",
    "# 说明：请根据实际 run_name/保存路径调整 RUN_NAME / CHECKPOINT_DIR 变量\n",
    "import os, yaml, glob, math, time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "from diffusers import DDIMScheduler, AutoencoderKL, UNet2DConditionModel\n",
    "\n",
    "# 项目内模块\n",
    "from model.pipeline import ImageFusionPipeline, ConditioningEncoder\n",
    "from dataset import ImageFusionDataset\n",
    "import metric\n",
    "\n",
    "# --- 配置区域：根据实际调整 ---\n",
    "RUN_NAME = None  # 若已知设置为 \"your_run_name\"，否则留 None 自动选择 checkpoints/pretrain 下最新的 run\n",
    "PRETRAIN_BASE = \"./checkpoints/pretrain/fusion_diffusion_pretrain_v1\"\n",
    "VAE_FALLBACK = \"./checkpoints/vae/best.pth\"\n",
    "NUM_SHOW_BATCHES = 1   # 每个数据集展示多少个批次\n",
    "NUM_INFERENCE_STEPS = 50\n",
    "\n",
    "# --- 自动查找 run_dir 和 config.yml ---\n",
    "if RUN_NAME is None:\n",
    "    cand = sorted(Path(PRETRAIN_BASE).glob(\"*\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(f\"No runs found under {PRETRAIN_BASE}. Set RUN_NAME manually.\")\n",
    "    run_dir = cand[0]\n",
    "else:\n",
    "    run_dir = Path(PRETRAIN_BASE) / RUN_NAME\n",
    "config_path = Path(PRETRAIN_BASE) / \"config.yml\"\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"Config not found at {config_path}\")\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Using run_dir = {run_dir}\")\n",
    "print(f\"Loaded config from {config_path}\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 构建模型结构并加载权重 ---\n",
    "# 1) VAE\n",
    "vae_cfg = config[\"model_config\"][\"vae\"]\n",
    "vae_init_kwargs = {k: v for k, v in vae_cfg.items() if k != \"checkpoint_dir\"}\n",
    "vae = AutoencoderKL(**vae_init_kwargs)\n",
    "vae_ckpt_path = vae_cfg.get(\"checkpoint_dir\", VAE_FALLBACK)\n",
    "if not os.path.exists(vae_ckpt_path):\n",
    "    raise FileNotFoundError(f\"VAE checkpoint not found at {vae_ckpt_path}\")\n",
    "sd = torch.load(vae_ckpt_path, map_location=\"cpu\")\n",
    "if isinstance(sd, dict) and \"model_state_dict\" in sd:\n",
    "    sd = sd[\"model_state_dict\"]\n",
    "vae.load_state_dict(sd, strict=True)\n",
    "vae.to(device).eval()\n",
    "scaling_factor = getattr(vae.config, \"scaling_factor\", None)\n",
    "if scaling_factor is None:\n",
    "    raise RuntimeError(\"vae.config.scaling_factor missing\")\n",
    "\n",
    "# 2) UNet & Encoder\n",
    "unet_cfg = config[\"model_config\"][\"unet\"]\n",
    "unet = UNet2DConditionModel(**unet_cfg)\n",
    "encoder = ConditioningEncoder(**config[\"model_config\"][\"encoder\"])\n",
    "\n",
    "# 寻找保存的 unet/encoder 权重（优先 final，再找 epoch_*）\n",
    "def find_weights(base):\n",
    "    final = Path(base) / \"final\"\n",
    "    if final.exists():\n",
    "        u = final / \"unet.pth\"\n",
    "        e = final / \"encoder.pth\"\n",
    "        if u.exists() and e.exists():\n",
    "            return u, e\n",
    "    # 寻找最新 epoch_x\n",
    "    epochs = sorted(Path(base).glob(\"epoch_*\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    for ep in epochs:\n",
    "        u = ep / \"unet.pth\"\n",
    "        e = ep / \"encoder.pth\"\n",
    "        if u.exists() and e.exists():\n",
    "            return u, e\n",
    "    # fallback to files directly under run_dir\n",
    "    u = Path(base) / \"unet.pth\"\n",
    "    e = Path(base) / \"encoder.pth\"\n",
    "    if u.exists() and e.exists():\n",
    "        return u, e\n",
    "    return None, None\n",
    "\n",
    "unet_path, encoder_path = find_weights(run_dir)\n",
    "if unet_path is None or encoder_path is None:\n",
    "    raise FileNotFoundError(f\"Could not find unet/encoder weights under {run_dir}\")\n",
    "unet.load_state_dict(torch.load(unet_path, map_location=\"cpu\"))\n",
    "encoder.load_state_dict(torch.load(encoder_path, map_location=\"cpu\"))\n",
    "unet.to(device).eval()\n",
    "encoder.to(device).eval()\n",
    "\n",
    "# scheduler for inference / for computing add_noise during proxy loss\n",
    "diffusion_cfg = config.get(\"diffusion\", {})\n",
    "num_train_timesteps = 1000\n",
    "scheduler = DDIMScheduler(num_train_timesteps=num_train_timesteps, beta_schedule=\"squaredcos_cap_v2\")\n",
    "\n",
    "# 构造 pipeline\n",
    "pipeline = ImageFusionPipeline(unet=unet, scheduler=scheduler, encoder=encoder, vae=vae, vae_scale_factor=4).to(device)\n",
    "\n",
    "# --- 数据集 loaders（参考 pretrain.py） ---\n",
    "train_ds_config = config[\"train_dataset\"]\n",
    "train_paths = config[\"datasets\"][train_ds_config[\"name\"]][\"train\"]\n",
    "train_dataset = ImageFusionDataset(dir_A=train_paths[\"dir_A\"], dir_B=train_paths[\"dir_B\"], dir_C=train_paths.get(\"dir_C\"), is_train=True, is_getpatch=False, augment=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=min(4, len(train_dataset) or 1), shuffle=False, num_workers=2)\n",
    "\n",
    "test_loaders = {}\n",
    "for test_set in config.get(\"test_sets\", []):\n",
    "    name = test_set[\"name\"]\n",
    "    test_paths = config[\"datasets\"][name][\"test\"]\n",
    "    ds = ImageFusionDataset(dir_A=test_paths[\"dir_A\"], dir_B=test_paths[\"dir_B\"], dir_C=test_paths.get(\"dir_C\"), is_train=False, is_getpatch=False)\n",
    "    test_loaders[name] = DataLoader(ds, batch_size=test_set.get(\"test_batch_size\", 4), shuffle=False)\n",
    "\n",
    "# --- metric 准备（优先使用 *_function_batch） ---\n",
    "metric_batch_funcs = {}\n",
    "metric_single_funcs = {}\n",
    "for name in dir(metric):\n",
    "    if name.endswith(\"_function_batch\") and callable(getattr(metric, name)):\n",
    "        metric_batch_funcs[name[:-15]] = getattr(metric, name)\n",
    "    if name.endswith(\"_function\") and callable(getattr(metric, name)):\n",
    "        metric_single_funcs[name[:-9]] = getattr(metric, name)\n",
    "all_metric_names = sorted(set(metric_batch_funcs.keys()) | set(metric_single_funcs.keys()))\n",
    "print(\"Metrics detected:\", all_metric_names)\n",
    "\n",
    "# 辅助：tensor -> uint8 image for display\n",
    "def tensor_to_uint8(img_tensor):\n",
    "    # expect [-1,1] float tensor (B, C, H, W)\n",
    "    img = img_tensor.clamp(-1,1).add(1).mul(127.5).cpu().numpy().astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "os.makedirs(\"./play_outputs\", exist_ok=True)\n",
    "\n",
    "# --- 函数：对一个 loader 做推理/度量/显示 ---\n",
    "def run_and_report(loader, name, max_batches=NUM_SHOW_BATCHES):\n",
    "    print(f\"\\n===== Dataset: {name} =====\")\n",
    "    results = []\n",
    "    batch_idx = 0\n",
    "    for batch in loader:\n",
    "        if batch_idx >= max_batches:\n",
    "            break\n",
    "        # batch may be tuple (vis, ir, label) or (vis, ir)\n",
    "        batch = tuple(t.to(device=device, dtype=torch.float32) for t in batch)\n",
    "        vis = batch[0]\n",
    "        ir = batch[1]\n",
    "        label = batch[2] if len(batch) > 2 else None\n",
    "\n",
    "        # inference\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            fused = pipeline(vis, ir, num_inference_steps=NUM_INFERENCE_STEPS)\n",
    "            elapsed = time.time() - start\n",
    "\n",
    "        # metrics: prefer batch implementations\n",
    "        metric_scores = {}\n",
    "        B = vis.shape[0]\n",
    "        # prepare metric args in \"pixel 0..255\" like pretrain\n",
    "        vis_m = ((vis.to(dtype=torch.float32) + 1.0) * 127.5).clamp(0,255)\n",
    "        ir_m = ((ir.to(dtype=torch.float32) + 1.0) * 127.5).clamp(0,255)\n",
    "        fused_m = ((fused.to(dtype=torch.float32) + 1.0) * 127.5).clamp(0,255)\n",
    "\n",
    "        for m in all_metric_names:\n",
    "            if m in metric_batch_funcs:\n",
    "                try:\n",
    "                    func = metric_batch_funcs[m]\n",
    "                    # try 3-arg, 2-arg, 1-arg\n",
    "                    import inspect\n",
    "                    sig = inspect.signature(func)\n",
    "                    params = len([p for p in sig.parameters.values() if p.kind in (p.POSITIONAL_ONLY, p.POSITIONAL_OR_KEYWORD)])\n",
    "                    if params >= 3:\n",
    "                        vals = func(vis_m, ir_m, fused_m)\n",
    "                    elif params == 2:\n",
    "                        vals = func(vis_m, fused_m)\n",
    "                    else:\n",
    "                        vals = func(fused_m)\n",
    "                    # normalize to numpy per-sample\n",
    "                    if isinstance(vals, torch.Tensor):\n",
    "                        arr = vals.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        arr = np.asarray(vals)\n",
    "                except Exception as e:\n",
    "                    print(f\"Metric {m} batch impl failed: {e}\")\n",
    "                    arr = np.full((B,), np.nan)\n",
    "            elif m in metric_single_funcs and label is not None:\n",
    "                # fallback to per-sample CPU functions (slower)\n",
    "                arr = []\n",
    "                func = metric_single_funcs[m]\n",
    "                for i in range(B):\n",
    "                    try:\n",
    "                        # per-sample expects HWC maybe; pass flattened tensors similar to batch form\n",
    "                        v = func(((vis[i:i+1]+1.0)*127.5).clamp(0,255), ((ir[i:i+1]+1.0)*127.5).clamp(0,255), ((fused[i:i+1]+1.0)*127.5).clamp(0,255))\n",
    "                        arr.append(v)\n",
    "                    except Exception:\n",
    "                        arr.append(np.nan)\n",
    "                arr = np.asarray(arr)\n",
    "            else:\n",
    "                arr = np.full((B,), np.nan)\n",
    "            metric_scores[m] = arr\n",
    "\n",
    "        # 训练 proxy loss (仅当 label 存在且 data 来自训练集)\n",
    "        losses = None\n",
    "        if label is not None:\n",
    "            with torch.no_grad():\n",
    "                # encode target latent\n",
    "                enc = vae.encode(label)\n",
    "                lat_target = getattr(enc, \"latent_dist\", None)\n",
    "                if lat_target is not None:\n",
    "                    try:\n",
    "                        lat_target = enc.latent_dist.sample()\n",
    "                    except Exception:\n",
    "                        lat_target = enc.latent_dist.mean\n",
    "                elif isinstance(enc, dict):\n",
    "                    lat_target = enc.get(\"latent_dist\", enc.get(\"sample\", torch.as_tensor(enc))).sample \\\n",
    "                                 if \"latent_dist\" in enc or \"sample\" in enc else torch.as_tensor(enc)\n",
    "                else:\n",
    "                    lat_target = enc\n",
    "                lat_target = lat_target.to(device)\n",
    "                lat_target = lat_target * scaling_factor\n",
    "\n",
    "                # random timesteps & noise (proxy estimate of training loss)\n",
    "                B = lat_target.shape[0]\n",
    "                ts = torch.randint(0, scheduler.config.num_train_timesteps, (B,), device=device).long()\n",
    "                noise = torch.randn_like(lat_target)\n",
    "                try:\n",
    "                    noisy = scheduler.add_noise(lat_target, noise, ts)\n",
    "                except Exception:\n",
    "                    noisy = lat_target + noise\n",
    "                # predicted noise by saved models\n",
    "                enc_cond = encoder(torch.cat([vis, ir], dim=1))\n",
    "                pred_noise = unet(noisy, ts, encoder_hidden_states=enc_cond).sample\n",
    "                # per-sample L1\n",
    "                per_sample = F.l1_loss(pred_noise, noise, reduction=\"none\")\n",
    "                # mean over non-batch dims\n",
    "                mean_dims = tuple(range(1, per_sample.dim()))\n",
    "                losses = per_sample.mean(dim=mean_dims).detach().cpu().numpy()\n",
    "        else:\n",
    "            losses = np.full((vis.shape[0],), np.nan)\n",
    "\n",
    "        # 打印 summary\n",
    "        print(f\"Batch {batch_idx} - elapsed {elapsed:.3f}s - metric samples count {B}\")\n",
    "        # 打印 metric averages for the batch\n",
    "        for m, arr in metric_scores.items():\n",
    "            mean_val = float(np.nanmean(arr)) if arr.size else float(\"nan\")\n",
    "            print(f\"  {m}: {mean_val:.4f}\")\n",
    "\n",
    "        # 打印 proxy losses\n",
    "        for i, l in enumerate(losses):\n",
    "            print(f\"  sample[{i}] proxy L1 loss: {l:.6f}\")\n",
    "\n",
    "        # 显示图片：将 fused / label / vis (只展示第一样本)\n",
    "        vis_uint8 = tensor_to_uint8(vis)\n",
    "        fused_uint8 = tensor_to_uint8(fused)\n",
    "        if label is not None:\n",
    "            label_uint8 = tensor_to_uint8(label)\n",
    "        B = vis_uint8.shape[0]\n",
    "        for i in range(B):\n",
    "            fig, axs = plt.subplots(1, 3 if label is not None else 2, figsize=(12,4))\n",
    "            axs[0].imshow(vis_uint8[i].transpose(1,2,0)); axs[0].set_title(\"VIS\"); axs[0].axis(\"off\")\n",
    "            axs[1].imshow(fused_uint8[i].transpose(1,2,0)); axs[1].set_title(\"Fused\"); axs[1].axis(\"off\")\n",
    "            if label is not None:\n",
    "                axs[2].imshow(label_uint8[i].transpose(1,2,0)); axs[2].set_title(\"Label\"); axs[2].axis(\"off\")\n",
    "            plt.suptitle(f\"{name} batch{batch_idx} sample{i}\")\n",
    "            plt.show()\n",
    "            out_path = f\"./play_outputs/{name}_b{batch_idx}_s{i}.png\"\n",
    "            plt.imsave(out_path, fused_uint8[i].transpose(1,2,0))\n",
    "        batch_idx += 1\n",
    "        results.append({\"metrics\": metric_scores, \"losses\": losses})\n",
    "    return results\n",
    "\n",
    "# --- 运行 train & test ---\n",
    "train_results = run_and_report(train_loader, \"train\", max_batches=NUM_SHOW_BATCHES)\n",
    "test_results = {}\n",
    "for tname, loader in test_loaders.items():\n",
    "    test_results[tname] = run_and_report(loader, tname, max_batches=NUM_SHOW_BATCHES)\n",
    "\n",
    "print(\"\\nAll done. Generated images saved to ./play_outputs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusionrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
